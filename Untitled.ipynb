{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d499b2b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124800, 28, 28)\n",
      "(124800,)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from emnist import extract_training_samples\n",
    "from emnist import extract_test_samples\n",
    "\n",
    "images, labels = extract_training_samples('letters')\n",
    "print(images.shape)\n",
    "print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b46b720b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4733, 784)\n",
      "(4733,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load images and labels\n",
    "X = np.loadtxt('cyrillic_data.csv', delimiter=\",\")\n",
    "y = np.loadtxt('cyrillic_label.csv', delimiter=\",\")\n",
    "y = y[:4733]\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=123)\n",
    "\n",
    "#print(X_train)\n",
    "#print(X_test)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71fbaf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying letter Я - 32.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying letter Ы - 28.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying letter М - 13.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAANzUlEQVR4nO3db6hc9Z3H8c/H2BqSVkk2GkIU7TYiq8LeaJBg6uJSWrJ5okWz1AeLyyq3Dyo0uuCGrlBhWdTddX3gg2JKpVntRqpGFJG2Ucq6iopRo0azUTe4af6YeE1C/miISb774J6UW71z5mbOOXMm832/4DIz5zvnzJdDPjln5jdzfo4IARh+p7XdAID+IOxAEoQdSIKwA0kQdiCJ0/v5Yrb56B9oWER4suWVjuy2l9rebPsD2yurbAtAs9zrOLvtaZLek/QdSdskvSrphoh4t2QdjuxAw5o4sl8h6YOI2BIRRyQ9IumaCtsD0KAqYZ8v6fcTHm8rlv0R26O219teX+G1AFRU5QO6yU4VvnSaHhGrJK2SOI0H2lTlyL5N0nkTHp8raUe1dgA0pUrYX5V0oe1v2P6qpO9LeqqetgDUrefT+Ig4avsWSb+RNE3SgxHxTm2dnaQFCxaU1ufOnVtaf/HFF+tsBxg4lb5UExHPSHqmpl4ANIivywJJEHYgCcIOJEHYgSQIO5AEYQeS6PlXbz29WINfl33sscdK68uWLSutn3322aX1Q4cOdazdfvvtpesuX768tD5jxozS+p49e0rrDzzwQMfaww8/XLouhk8jv2cHcOog7EAShB1IgrADSRB2IAnCDiTR10tJN2nTpk2l9euuu660fv7555fWL7rooo61e+65p3TdTz75pLS+a9eu0vrIyEhp/aGHHupYu/zyy0vXvfXWW0vrZ555Zmn9tttuK60vXLiwY+2MM84oXbfbsPCzzz5bWr/33ntL69lwZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJIZmnP3YsWONbr9sPPnw4cOl61566aWl9Y8++qi0PmvWrNL6k08+2bG2YsWK0nW7XUL7rrvuKq13u4T30aNHO9Y+/fTT0nWnT59eWl+6dGlp/ZVXXulYe+GFF0rXHUYc2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgiaEZZ6+q25TOV155Zcdat8tYdxtH72bv3r2l9ZtvvrljbfPmzaXrPvLII6X1adOmlda7/Z79/vvv71grG4OXpMWLF5fWX3rppdL6VVdd1bGWcZy9UthtfyjpgKRjko5GxKI6mgJQvzqO7H8ZEWM1bAdAg3jPDiRRNewh6be2X7M9OtkTbI/aXm97fcXXAlBB1dP4JRGxw/Y5ktbZ/p+IeH7iEyJilaRVUrNzvQEoV+nIHhE7itvdkp6QdEUdTQGoX89htz3T9tdP3Jf0XUkb62oMQL2qnMbPlfSE7RPb+c+I+HUtXbXg+uuvL62fdlrn/xfXrl1bdzsn5b333utYe/fdd0vXvfjii0vrZb+Vl6T77ruvtF7F/v37K63f7br02fQc9ojYIunPa+wFQIMYegOSIOxAEoQdSIKwA0kQdiAJfuJa6Dal82effdax9vTTT9fdTm3Gxqr9RumOO+6oqZOTt2/fvkrrd7sEdzYc2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcbZC90uJb1mzZqOtUOHDtXdzkkpG09esmRJpW0fOHCg0vpVHDx4sNL6M2fOrKmT4cCRHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSGJpx9uPHjze6/W5TG7fpsssu61jrNuVyN4xVDw+O7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQxNCMs1ed3rfsuvCStG7dukrbb9L8+fPbbgGngK5HdtsP2t5te+OEZbNtr7P9fnHL1fiBATeV0/hfSFr6hWUrJT0XERdKeq54DGCAdQ17RDwvac8XFl8jaXVxf7Wka+ttC0Dden3PPjcidkpSROy0fU6nJ9oelTTa4+sAqEnjH9BFxCpJqyTJdjT9egAm1+vQ2y7b8ySpuN1dX0sAmtBr2J+SdGNx/0ZJT9bTDoCmdD2Nt71G0tWS5tjeJuknku6W9CvbN0naKml5k01ORdVrjL/xxhul9W7j8G1asGBB2y3gFNA17BFxQ4fSt2vuBUCD+LoskARhB5Ig7EAShB1IgrADSQzNT1yPHTtWaf2XX365pk76b968eY1tu+p+xeDgyA4kQdiBJAg7kARhB5Ig7EAShB1IgrADSQzNOPu+ffsqrX/gwIF6GmmB7ca2XXW/YnBwZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJIZmnH3v3r1tt3BK+vzzz0vrY2NjfeoETePIDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJDM04++HDhyutP2PGjJo6ObVs2bKltN7mdeOnT59eaf0jR47U1Mlw6Hpkt/2g7d22N05Ydqft7bY3FH/Lmm0TQFVTOY3/haSlkyy/LyJGir9n6m0LQN26hj0inpe0pw+9AGhQlQ/obrH9VnGaP6vTk2yP2l5ve32F1wJQUa9h/6mkb0oakbRT0r2dnhgRqyJiUUQs6vG1ANSgp7BHxK6IOBYRxyX9TNIV9bYFoG49hd32xDmCvydpY6fnAhgMXcfZba+RdLWkOba3SfqJpKttj0gKSR9K+kFzLfbH6acPzVcOTsqbb77ZdgsdzZkzp9L6H3/8cU2dDIeu/8Ij4oZJFv+8gV4ANIivywJJEHYgCcIOJEHYgSQIO5BEzvEm/EG3n7ieyiKi7RYGCkd2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCcfbk9u/f33YL6BOO7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOPsQ6DK77bHxsZq7ASDjCM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiQxNOPs3caLH3300dL6hg0bauymv7Zv397zusePH6+xk3odPXq00vpnnXVWTZ0Mh65Hdtvn2f6d7U2237H9o2L5bNvrbL9f3M5qvl0AvZrKafxRSX8fEX8mabGkH9q+WNJKSc9FxIWSniseAxhQXcMeETsj4vXi/gFJmyTNl3SNpNXF01ZLurahHgHU4KTes9u+QNJCSa9ImhsRO6Xx/xBsn9NhnVFJoxX7BFDRlMNu+2uSHpe0IiL2257SehGxStKqYhvMtAe0ZEpDb7a/ovGg/zIi1haLd9meV9TnSdrdTIsA6uBuP4/0+CF8taQ9EbFiwvJ/lfRJRNxte6Wk2RFxe5dtcWRvwLnnntuxdskll5Suu3HjxtJ6lWG9qrqdPW7durW0Xtb74sWLe+rpVBARk+64qZzGL5H0N5Letr2hWPZjSXdL+pXtmyRtlbS8hj4BNKRr2CPiBUmd/ov9dr3tAGgKX5cFkiDsQBKEHUiCsANJEHYgia7j7LW+GOPsqNHIyEjP657KP2nuptM4O0d2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCcXZgyDDODiRH2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0l0Dbvt82z/zvYm2+/Y/lGx/E7b221vKP6WNd8ugF51vXiF7XmS5kXE67a/Luk1SddK+mtJByPi36b8Yly8Amhcp4tXTGV+9p2Sdhb3D9jeJGl+ve0BaNpJvWe3fYGkhZJeKRbdYvst2w/antVhnVHb622vr9YqgCqmfA0621+T9F+S/jki1tqeK2lMUkj6J42f6v9dl21wGg80rNNp/JTCbvsrkp6W9JuI+PdJ6hdIejoiLu2yHcIONKznC07atqSfS9o0MejFB3cnfE/SxqpNAmjOVD6N/5ak/5b0tqTjxeIfS7pB0ojGT+M/lPSD4sO8sm1xZAcaVuk0vi6EHWge140HkiPsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4k0fWCkzUbk/R/Ex7PKZYNokHtbVD7kuitV3X2dn6nQl9/z/6lF7fXR8Si1hooMai9DWpfEr31ql+9cRoPJEHYgSTaDvuqll+/zKD2Nqh9SfTWq7701up7dgD90/aRHUCfEHYgiVbCbnup7c22P7C9so0eOrH9oe23i2moW52frphDb7ftjROWzba9zvb7xe2kc+y11NtATONdMs14q/uu7enP+/6e3fY0Se9J+o6kbZJelXRDRLzb10Y6sP2hpEUR0foXMGz/haSDkv7jxNRatv9F0p6IuLv4j3JWRPzDgPR2p05yGu+Geus0zfjfqsV9V+f0571o48h+haQPImJLRByR9Iika1roY+BFxPOS9nxh8TWSVhf3V2v8H0vfdehtIETEzoh4vbh/QNKJacZb3XclffVFG2GfL+n3Ex5v02DN9x6Sfmv7NdujbTczibknptkqbs9puZ8v6jqNdz99YZrxgdl3vUx/XlUbYZ9sappBGv9bEhGXSforST8sTlcxNT+V9E2NzwG4U9K9bTZTTDP+uKQVEbG/zV4mmqSvvuy3NsK+TdJ5Ex6fK2lHC31MKiJ2FLe7JT2h8bcdg2TXiRl0i9vdLffzBxGxKyKORcRxST9Ti/uumGb8cUm/jIi1xeLW991kffVrv7UR9lclXWj7G7a/Kun7kp5qoY8vsT2z+OBEtmdK+q4GbyrqpyTdWNy/UdKTLfbyRwZlGu9O04yr5X3X+vTnEdH3P0nLNP6J/P9K+sc2eujQ159KerP4e6ft3iSt0fhp3ecaPyO6SdKfSHpO0vvF7ewB6u0hjU/t/ZbGgzWvpd6+pfG3hm9J2lD8LWt735X01Zf9xtdlgST4Bh2QBGEHkiDsQBKEHUiCsANJEHYgCcIOJPH/Z5dNhtqoaMwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying letter Ч - 24.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying letter Ц - 23.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display 5 random letters and their labels\n",
    "for _ in range(5):\n",
    "    i = np.random.randint(X.shape[0])\n",
    "    my_letter = X[i].reshape(28,28)\n",
    "    UC = ''.join([chr(ord(i)+975)for i in\"ABCDEF2GHIJKLMNOPQRSTUVWXYZ[\\\\]^_`\"]) + \"I\"\n",
    "    my_label = UC[int(y[i])]\n",
    "    print(f\"Displaying letter {my_label} - {y[i]}\")\n",
    "    plt.imshow(my_letter, cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a96a561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "from emnist import extract_training_samples\n",
    "from emnist import extract_test_samples\n",
    "\n",
    "def load_data_symbols():\n",
    "    X_train, y_train = extract_training_samples('letters')\n",
    "    X_test, y_test = extract_test_samples('letters')\n",
    "    \n",
    "    training_data = (X_train, y_train)\n",
    "    test_data = (X_test, y_test)\n",
    "    \n",
    "    return (training_data, test_data)\n",
    "    \n",
    "def load_data_wrapper_symbols():\n",
    "    tr_d, te_d = load_data_symbols()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result_symbols(int(y)) for y in tr_d[1]]\n",
    "    training_data = list(zip(training_inputs, training_results))\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = list(zip(test_inputs, te_d[1]))\n",
    "    return (training_data, test_data)\n",
    "\n",
    "def load_data_cyrillic():\n",
    "    X = np.loadtxt('cyrillic_data.csv', delimiter=\",\")\n",
    "    y = np.loadtxt('cyrillic_label.csv', delimiter=\",\")\n",
    "    y = y[:4733]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=123)\n",
    "    \n",
    "    training_data = (X_train, y_train)\n",
    "    test_data = (X_test, y_test)\n",
    "    \n",
    "    return (training_data, test_data)\n",
    "\n",
    "def load_data_wrapper_cyrillic():\n",
    "    tr_d, te_d = load_data_cyrillic()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result_cyrillic(int(y)) for y in tr_d[1]]\n",
    "    training_data = list(zip(training_inputs, training_results))\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = list(zip(test_inputs, te_d[1]))\n",
    "    return (training_data, test_data)\n",
    "\n",
    "def load_data_value():\n",
    "    f = gzip.open('mnist2.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='bytes')\n",
    "    print(training_data)\n",
    "    print(type(training_data))\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper_value():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = list(zip(training_inputs, training_results))\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = list(zip(validation_inputs, va_d[1]))\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = list(zip(test_inputs, te_d[1]))\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result_cyrillic(j):\n",
    "    e = np.zeros((33, 1))\n",
    "    e[j-1] = 1.0\n",
    "    return e\n",
    "\n",
    "a, b = load_data_wrapper_cyrillic()\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "\n",
    "def vectorized_result_symbols(j):\n",
    "    e = np.zeros((26, 1))\n",
    "    e[j-1] = 1.0\n",
    "    return e\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "557e22f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 194 / 1894\n",
      "Epoch 0 complete\n",
      "Epoch 1: 259 / 1894\n",
      "Epoch 1 complete\n",
      "Epoch 2: 307 / 1894\n",
      "Epoch 2 complete\n",
      "Epoch 3: 353 / 1894\n",
      "Epoch 3 complete\n",
      "Epoch 4: 395 / 1894\n",
      "Epoch 4 complete\n",
      "Epoch 5: 458 / 1894\n",
      "Epoch 5 complete\n",
      "Epoch 6: 495 / 1894\n",
      "Epoch 6 complete\n",
      "Epoch 7: 533 / 1894\n",
      "Epoch 7 complete\n",
      "Epoch 8: 563 / 1894\n",
      "Epoch 8 complete\n",
      "Epoch 9: 603 / 1894\n",
      "Epoch 9 complete\n",
      "Epoch 10: 642 / 1894\n",
      "Epoch 10 complete\n",
      "Epoch 11: 666 / 1894\n",
      "Epoch 11 complete\n",
      "Epoch 12: 711 / 1894\n",
      "Epoch 12 complete\n",
      "Epoch 13: 739 / 1894\n",
      "Epoch 13 complete\n",
      "Epoch 14: 775 / 1894\n",
      "Epoch 14 complete\n",
      "Epoch 15: 824 / 1894\n",
      "Epoch 15 complete\n",
      "Epoch 16: 836 / 1894\n",
      "Epoch 16 complete\n",
      "Epoch 17: 873 / 1894\n",
      "Epoch 17 complete\n",
      "Epoch 18: 891 / 1894\n",
      "Epoch 18 complete\n",
      "Epoch 19: 921 / 1894\n",
      "Epoch 19 complete\n",
      "Epoch 20: 945 / 1894\n",
      "Epoch 20 complete\n",
      "Epoch 21: 974 / 1894\n",
      "Epoch 21 complete\n",
      "Epoch 22: 989 / 1894\n",
      "Epoch 22 complete\n",
      "Epoch 23: 1010 / 1894\n",
      "Epoch 23 complete\n",
      "Epoch 24: 1042 / 1894\n",
      "Epoch 24 complete\n",
      "Epoch 25: 1039 / 1894\n",
      "Epoch 25 complete\n",
      "Epoch 26: 1067 / 1894\n",
      "Epoch 26 complete\n",
      "Epoch 27: 1068 / 1894\n",
      "Epoch 27 complete\n",
      "Epoch 28: 1098 / 1894\n",
      "Epoch 28 complete\n",
      "Epoch 29: 1107 / 1894\n",
      "Epoch 29 complete\n",
      "Epoch 30: 1117 / 1894\n",
      "Epoch 30 complete\n",
      "Epoch 31: 1127 / 1894\n",
      "Epoch 31 complete\n",
      "Epoch 32: 1141 / 1894\n",
      "Epoch 32 complete\n",
      "Epoch 33: 1142 / 1894\n",
      "Epoch 33 complete\n",
      "Epoch 34: 1160 / 1894\n",
      "Epoch 34 complete\n",
      "Epoch 35: 1158 / 1894\n",
      "Epoch 35 complete\n",
      "Epoch 36: 1175 / 1894\n",
      "Epoch 36 complete\n",
      "Epoch 37: 1181 / 1894\n",
      "Epoch 37 complete\n",
      "Epoch 38: 1191 / 1894\n",
      "Epoch 38 complete\n",
      "Epoch 39: 1188 / 1894\n",
      "Epoch 39 complete\n",
      "Epoch 40: 1200 / 1894\n",
      "Epoch 40 complete\n",
      "Epoch 41: 1210 / 1894\n",
      "Epoch 41 complete\n",
      "Epoch 42: 1205 / 1894\n",
      "Epoch 42 complete\n",
      "Epoch 43: 1221 / 1894\n",
      "Epoch 43 complete\n",
      "Epoch 44: 1229 / 1894\n",
      "Epoch 44 complete\n",
      "Epoch 45: 1233 / 1894\n",
      "Epoch 45 complete\n",
      "Epoch 46: 1242 / 1894\n",
      "Epoch 46 complete\n",
      "Epoch 47: 1241 / 1894\n",
      "Epoch 47 complete\n",
      "Epoch 48: 1244 / 1894\n",
      "Epoch 48 complete\n",
      "Epoch 49: 1249 / 1894\n",
      "Epoch 49 complete\n",
      "Epoch 50: 1255 / 1894\n",
      "Epoch 50 complete\n",
      "Epoch 51: 1265 / 1894\n",
      "Epoch 51 complete\n",
      "Epoch 52: 1268 / 1894\n",
      "Epoch 52 complete\n",
      "Epoch 53: 1270 / 1894\n",
      "Epoch 53 complete\n",
      "Epoch 54: 1273 / 1894\n",
      "Epoch 54 complete\n",
      "Epoch 55: 1277 / 1894\n",
      "Epoch 55 complete\n",
      "Epoch 56: 1298 / 1894\n",
      "Epoch 56 complete\n",
      "Epoch 57: 1296 / 1894\n",
      "Epoch 57 complete\n",
      "Epoch 58: 1284 / 1894\n",
      "Epoch 58 complete\n",
      "Epoch 59: 1305 / 1894\n",
      "Epoch 59 complete\n",
      "Epoch 60: 1303 / 1894\n",
      "Epoch 60 complete\n",
      "Epoch 61: 1308 / 1894\n",
      "Epoch 61 complete\n",
      "Epoch 62: 1309 / 1894\n",
      "Epoch 62 complete\n",
      "Epoch 63: 1320 / 1894\n",
      "Epoch 63 complete\n",
      "Epoch 64: 1316 / 1894\n",
      "Epoch 64 complete\n",
      "Epoch 65: 1323 / 1894\n",
      "Epoch 65 complete\n",
      "Epoch 66: 1325 / 1894\n",
      "Epoch 66 complete\n",
      "Epoch 67: 1331 / 1894\n",
      "Epoch 67 complete\n",
      "Epoch 68: 1332 / 1894\n",
      "Epoch 68 complete\n",
      "Epoch 69: 1330 / 1894\n",
      "Epoch 69 complete\n",
      "Epoch 70: 1332 / 1894\n",
      "Epoch 70 complete\n",
      "Epoch 71: 1339 / 1894\n",
      "Epoch 71 complete\n",
      "Epoch 72: 1346 / 1894\n",
      "Epoch 72 complete\n",
      "Epoch 73: 1348 / 1894\n",
      "Epoch 73 complete\n",
      "Epoch 74: 1353 / 1894\n",
      "Epoch 74 complete\n",
      "Epoch 75: 1353 / 1894\n",
      "Epoch 75 complete\n",
      "Epoch 76: 1363 / 1894\n",
      "Epoch 76 complete\n",
      "Epoch 77: 1364 / 1894\n",
      "Epoch 77 complete\n",
      "Epoch 78: 1358 / 1894\n",
      "Epoch 78 complete\n",
      "Epoch 79: 1364 / 1894\n",
      "Epoch 79 complete\n",
      "Epoch 80: 1375 / 1894\n",
      "Epoch 80 complete\n",
      "Epoch 81: 1369 / 1894\n",
      "Epoch 81 complete\n",
      "Epoch 82: 1371 / 1894\n",
      "Epoch 82 complete\n",
      "Epoch 83: 1372 / 1894\n",
      "Epoch 83 complete\n",
      "Epoch 84: 1374 / 1894\n",
      "Epoch 84 complete\n",
      "Epoch 85: 1378 / 1894\n",
      "Epoch 85 complete\n",
      "Epoch 86: 1379 / 1894\n",
      "Epoch 86 complete\n",
      "Epoch 87: 1378 / 1894\n",
      "Epoch 87 complete\n",
      "Epoch 88: 1380 / 1894\n",
      "Epoch 88 complete\n",
      "Epoch 89: 1378 / 1894\n",
      "Epoch 89 complete\n",
      "Epoch 90: 1381 / 1894\n",
      "Epoch 90 complete\n",
      "Epoch 91: 1383 / 1894\n",
      "Epoch 91 complete\n",
      "Epoch 92: 1381 / 1894\n",
      "Epoch 92 complete\n",
      "Epoch 93: 1384 / 1894\n",
      "Epoch 93 complete\n",
      "Epoch 94: 1383 / 1894\n",
      "Epoch 94 complete\n",
      "Epoch 95: 1383 / 1894\n",
      "Epoch 95 complete\n",
      "Epoch 96: 1392 / 1894\n",
      "Epoch 96 complete\n",
      "Epoch 97: 1393 / 1894\n",
      "Epoch 97 complete\n",
      "Epoch 98: 1384 / 1894\n",
      "Epoch 98 complete\n",
      "Epoch 99: 1393 / 1894\n",
      "Epoch 99 complete\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "T0 = -1\n",
    "\n",
    "# Определение логистической сигмоиды\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z*alpha))\n",
    "\n",
    "# Производная сигмоидальной функции\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "# Определить структуру нейронной сети\n",
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        # Сетевой уровень\n",
    "        self.num_layers = len(sizes)\n",
    "        # Количество нейронов в каждом слое\n",
    "        self.sizes = sizes\n",
    "        # Инициализировать смещение каждого слоя\n",
    "        #self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.biases = [np.random.randn(y, 1)*0 + T0 for y in sizes[1:]]\n",
    "\n",
    "        # Инициализировать вес каждого слоя\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "\n",
    "    #Градиентный спуск\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        if test_data:\n",
    "            n_test = len(test_data)\n",
    "        # Общее количество обучающих данных\n",
    "        n = len(training_data)\n",
    "\n",
    "        # Начните тренироваться, проходите каждую эпоху\n",
    "        for j in range(epochs):  #Xrange в python2.7\n",
    "            # Перемешать данные тренировки\n",
    "            random.shuffle(training_data)\n",
    "\n",
    "            # mini_batch\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "\n",
    "            # Обучение mini_batch\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "\n",
    "            if test_data:\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data), n_test))\n",
    "            print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        # Сохранить каждый слой частичного руководства\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        # Поезд mini_batch\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.update(x, y)\n",
    "\n",
    "            # Сохранить частную производную каждого слоя в обучающей сети один раз\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "\n",
    "        # Обновление веса и смещения Wn + 1 = Wn-eta * nw\n",
    "        self.weights = [w - (eta / len(mini_batch)) * nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta / len(mini_batch)) * nb for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    # Прямое распространение\n",
    "    def update(self, x, y):\n",
    "        # Сохранить каждый слой частичного руководства\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        activation = x  # Сохраненный ввод (данные обучения)\n",
    "\n",
    "        # Сохранить значение возбуждения каждого слоя a = sigmoid (z)\n",
    "        activations = [x]\n",
    "\n",
    "        # Сохранить каждый слой z = wx + b\n",
    "        zs = []\n",
    "        # Прямое распространение\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            # Рассчитать z каждого слоя\n",
    "            z = np.dot(w, activation) + b\n",
    "\n",
    "            # Сохранить z каждого слоя\n",
    "            zs.append(z)\n",
    "\n",
    "            # Рассчитать для каждого слоя\n",
    "            activation = sigmoid(z)\n",
    "\n",
    "            # Сохранить каждый слой\n",
    "            activations.append(activation)\n",
    "        # Обратное обновление\n",
    "        # Рассчитать погрешность последнего слоя\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "\n",
    "        # Производные весов и смещений последнего слоя\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].T)\n",
    "\n",
    "        # Предпоследний слой вплоть до первого уровня. Производные весов и смещений.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "\n",
    "            sp = sigmoid_prime(z)\n",
    "\n",
    "            # Ошибка текущего слоя\n",
    "            delta = np.dot(self.weights[-l+1].T, delta) * sp\n",
    "\n",
    "            # Производные смещения и веса текущего слоя\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].T)\n",
    "\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "        return  sum(int(x == y) for x, y in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activation, y):\n",
    "        return (output_activation - y)\n",
    "    \n",
    "#net = Network([2, 1])\n",
    "\n",
    "#print('Network net: ')\n",
    "#print('Count layers: ', net.num_layers)\n",
    "#for i in range(net.num_layers):\n",
    "#    print(\"Count neuron in layer: \", i, ' : ', net.sizes[i])\n",
    "#for i in range(net.num_layers - 1):\n",
    "#    print('W_', i+1, ' : ')\n",
    "#    print(np.round(net.weights[i], 2))\n",
    "#    print('b_', i+1, ' : ')\n",
    "#    print(np.round(net.biases[i], 2))\n",
    "\n",
    "training_data, test_data = load_data_wrapper_cyrillic()\n",
    "\n",
    "# 28 * 28 = 784 пикселей, вы можете определить 30 нейронов, всего 10 категорий\n",
    "net = Network([784, 30, 34])\n",
    "net.SGD(training_data, 100, 34, 0.5, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16feaa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, gzip, numpy as np, urllib.request\n",
    "from collections import namedtuple\n",
    "\"\"\"\n",
    "This file is for loading preprocessed MNIST dataset.\n",
    "\"\"\"\n",
    "Dataset = namedtuple('Dataset', ['X', 'y'])\n",
    "\n",
    "class MNISTPreprocessor:\n",
    "\tdef loadAndPreprocessDatasets(self, verbose=False):\n",
    "\t\t#\tDownload the data if not exists:\n",
    "\t\tif not os.path.isfile(\"mnist.pkl.gz\"):\n",
    "\t\t\tself.log(\"Downloading MNIST ...\", verbose)\n",
    "\t\t\tdata_url = \"http://deeplearning.net/data/mnist/mnist.pkl.gz\"\n",
    "\t\t\turllib.request.urlretrieve(data_url, \"mnist.pkl.gz\")\n",
    "\t\t\t\n",
    "\t\t\tself.log(\"Finished downloading MNIST ! [ Notice: Next run the script will use the already downloaded file ]\", verbose)\n",
    "\t\t\n",
    "\t\t#\tLoad the data:\n",
    "\t\twith gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "\t\t\tself.log(\"Found already downloaded MNIST file `{filename}`. Loading it ...\".format(filename=\"mnist.pkl.gz\"), verbose)\n",
    "\t\t\ttrain_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "\t\t\t\n",
    "\t\t#\tPre-process the data:\n",
    "\t\tself.log(\"Subtracting the mean of the train set from each of the 3 data-sets.\", verbose)\n",
    "\t\ttrain_set, valid_set, test_set = self.zeroMean(train_set, valid_set, test_set)\n",
    "\t\t# train_set, valid_set, test_set = self.unitVariance(train_set, valid_set, test_set)\n",
    "\t\t\n",
    "\t\tself.log(\"Setting the labels to be in {-1, 1}.\", verbose)\n",
    "\t\tdatasets = {\n",
    "\t\t\t'train': Dataset(train_set[0], self.convertToOneHot(train_set[1], 10)),\n",
    "\t\t\t'valid': Dataset(valid_set[0], self.convertToOneHot(valid_set[1], 10)),\n",
    "\t\t\t'test': Dataset(test_set[0], self.convertToOneHot(test_set[1], 10))\n",
    "\t\t}\n",
    "\t\treturn datasets\n",
    "\t\n",
    "\t@staticmethod\n",
    "\tdef processY(rawY):\n",
    "\t\tif len(rawY.shape) < 2:\n",
    "\t\t\trawY = rawY[:, np.newaxis]\n",
    "\t\treturn 2 * (rawY % 2) - 1\n",
    "\t\n",
    "\t@staticmethod\n",
    "\tdef convertToOneHot(y, numOfCategories):\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\t:param y: categorical vector y. assume y is a vector of ints (7,3,1,5..) to convert to one hot.\n",
    "\t\t:param numOfCategories: the number of categories in the one-hot vector\n",
    "\t\t:return:\n",
    "\t\t\"\"\"\n",
    "\t\toneHot = np.zeros((y.size, numOfCategories))\n",
    "\t\t\n",
    "\t\tfor i in range(y.size):\n",
    "\t\t\toneHot[i, y[i]] = 1\n",
    "\t\t\n",
    "\t\treturn oneHot\n",
    "\t\n",
    "\t@staticmethod\n",
    "\tdef zeroMean(train, validation, test):\n",
    "\t\t\"\"\"\n",
    "\t\tCalculate the mean of the samples of the train set, and transform the train set to 0 mean.\n",
    "\t\tUse the same mean to zero the mean of the test set (approximately).\n",
    "\t\tAlso process Y to -1, 1 for odd and even numbers.\n",
    "\t\t\"\"\"\n",
    "\t\ttrainMean = np.mean(train[0], axis=0)\n",
    "\t\tshift = lambda data_set: (data_set[0] - trainMean, data_set[1])\n",
    "\t\treturn shift(train), shift(validation), shift(test)\n",
    "\t\n",
    "\t@staticmethod\n",
    "\tdef unitVariance(train, validation, test):\n",
    "\t\t\"\"\"\n",
    "\t\tmake the data in [-1, 1]. Assume the data has 0 mean (i.e zeroMean already processed the data)\n",
    "\t\t:param train:\n",
    "\t\t:param validation:\n",
    "\t\t:param test:\n",
    "\t\t:return:\n",
    "\t\t\"\"\"\n",
    "\t\tmaxv = np.max(train[0])\n",
    "\t\tminv = np.min(train[0])\n",
    "\t\tm = max(abs(maxv), abs(minv))\n",
    "\t\tnormalize = lambda data_set: (data_set[0] / m, data_set[1])\n",
    "\t\treturn normalize(train), normalize(validation), normalize(test)\n",
    "\t\t\n",
    "\t\n",
    "\t@staticmethod\n",
    "\tdef log(msg, verbose=True):\n",
    "\t\tif verbose:\n",
    "\t\t\tprint(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69f0f1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emnist\n",
      "  Downloading emnist-0.0-py3-none-any.whl (7.3 kB)\n",
      "Requirement already satisfied: requests in c:\\anaconda3\\lib\\site-packages (from emnist) (2.25.1)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda3\\lib\\site-packages (from emnist) (4.59.0)\n",
      "Requirement already satisfied: numpy in c:\\anaconda3\\lib\\site-packages (from emnist) (1.20.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\anaconda3\\lib\\site-packages (from requests->emnist) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests->emnist) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests->emnist) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests->emnist) (1.26.4)\n",
      "Installing collected packages: emnist\n",
      "Successfully installed emnist-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install emnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06463533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading emnist.zip: 536MB [01:33, 6.03MB/s]                            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['balanced', 'byclass', 'bymerge', 'digits', 'letters', 'mnist']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from emnist import list_datasets\n",
    "list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51ef40bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614, 0.2550690257394217, 0.49543508709194095, 0.4494910647887381, 0.651592972722763, 0.7887233511355132, 0.0938595867742349, 0.02834747652200631, 0.8357651039198697, 0.43276706790505337, 0.762280082457942, 0.0021060533511106927, 0.4453871940548014, 0.7215400323407826, 0.22876222127045265, 0.9452706955539223, 0.9014274576114836, 0.030589983033553536, 0.0254458609934608, 0.5414124727934966, 0.9391491627785106, 0.38120423768821243, 0.21659939713061338, 0.4221165755827173, 0.029040787574867943, 0.22169166627303505, 0.43788759365057206, 0.49581224138185065, 0.23308445025757263, 0.2308665415409843, 0.2187810373376886, 0.4596034657377336, 0.28978161459048557, 0.021489705265908876, 0.8375779756625729, 0.5564543226524334, 0.6422943629324456, 0.1859062658947177, 0.9925434121760651, 0.8599465287952899, 0.12088995980580641, 0.3326951853601291, 0.7214844075832684, 0.7111917696952796, 0.9364405867994596, 0.4221069999614152, 0.830035693274327, 0.670305566414071, 0.3033685109329176, 0.5875806061435594, 0.8824790008318577, 0.8461974184283128, 0.5052838205796004, 0.5890022579825517, 0.034525830151341586, 0.24273997354306764, 0.7974042475543028, 0.4143139993007743, 0.17300740157905092, 0.548798761388153, 0.7030407620656315, 0.6744858305023272, 0.3747030205016403, 0.4389616300445631, 0.5084264882499818, 0.7784426150001458, 0.5209384176131452, 0.39325509496422606, 0.4896935204622582, 0.029574963966907064, 0.04348729035652743, 0.703382088603836, 0.9831877173096739, 0.5931837303800576, 0.393599686377914, 0.17034919685568128, 0.5022385584334831, 0.9820766375385342, 0.7705231398308006, 0.5396174484497788, 0.8602897789205496, 0.23217612806301458, 0.513771663187637, 0.9524673882682695, 0.5777948078012031, 0.45913173191066836, 0.2692794774414212, 0.5479963094662489, 0.9571162814602269, 0.005709129450392925, 0.7836552326153898, 0.8204859119254819, 0.8861795808260082, 0.7405034118331963, 0.8091399008724796, 0.518678283523002, 0.561357864778379, 0.4260906796881502, 0.05612329752074041, 0.8700101551766398, 0.5699993338763802, 0.19983942017714307, 0.5047204674288633, 0.48492511222773416, 0.3567899645449557, 0.3460779190181549, 0.5384787957378443, 0.6234894527975051, 0.6124524647827256, 0.4581468000997244, 0.027974984083842358, 0.22960503127702392, 0.1772112589385827, 0.5844608707784413, 0.8610088608533248, 0.798438940577426, 0.7970975626354962, 0.8164373705606909, 0.25529404008730594, 0.841744832274096, 0.6731135254387071, 0.08323413780389788, 0.0166906301155596, 0.014559974924812313, 0.7555867752521982, 0.2495592256534228, 0.10948862729435938, 0.6248020841524763, 0.3444228640964949, 0.06951537853084733, 0.1596255246938475, 0.5273803990480128, 0.16814494622242826, 0.2729144368186801, 0.7115899271852729, 0.4547016300456639, 0.3220017663873259, 0.4737710141702789, 0.023634577631987064, 0.38655710476146987, 0.4209186792090759, 0.18803930475131292, 0.10876169244541334, 0.8998185003560202, 0.5101159809286764, 0.2090909925517701, 0.6056486400340165, 0.8170396683778869, 0.020818108509287336, 0.017864520827795327, 0.146461740399346, 0.7188354727617898, 0.16022759262970465, 0.7046056278520025, 0.6781757952769475, 0.5447021635789044, 0.22059974802267657, 0.9755945178178834, 0.797810857706151, 0.516599516949393, 0.22319578024667075, 0.6485064180992564, 0.3948980098582996, 0.5758459627880567, 0.32124580934512525, 0.6309478612713469, 0.058785116206491295, 0.29860594962301334, 0.9679033101508892, 0.8755342442351592, 0.30638662033324593, 0.8585144063565593, 0.31036362735313405, 0.9392884321352825, 0.7438421186671211, 0.4161722627650255, 0.25235810227983535, 0.008480262463668842, 0.8787178982088466, 0.03791653059858058, 0.8194141106127972, 0.962201125180818, 0.5702805702451802, 0.17151709517771863, 0.8677810644349934, 0.9737752361596916, 0.7040231423300713, 0.5088737460778905, 0.37796883434360806, 0.34693088456262167, 0.2057617572947047, 0.6741530142468641, 0.4329501211003163, 0.1941186449851896, 0.10442422284151531, 0.6659575282786826, 0.29607267308315155, 0.4997999222368016, 0.3253456548759963, 0.8716215074235552, 0.8996782696347811, 0.018092983640471738, 0.2008530114407594, 0.3277407050962675, 0.9870497179280261, 0.7827003757293756, 0.3390956478509337, 0.21302979638081376, 0.6744550697237632, 0.8377010701539643, 0.9321874718936273, 0.3438498147908198, 0.8823932024664636, 0.6871101821536574, 0.48449872261249405, 0.9855082298257978, 0.23464043487103847, 0.7254651862412724, 0.0846802304164842, 0.16969414179438758, 0.9109877835080679, 0.21296819499142416, 0.7591161827164402, 0.6002088301322496, 0.8411321957058551, 0.3681079994056491, 0.34028523500198804, 0.29121528741113467, 0.8674198235869027, 0.6039825288917112, 0.9543074571721899, 0.8872651047169627, 0.13534597739545295, 0.5511704740692165, 0.1042749980146136, 0.03913779859691058, 0.07319341883234853, 0.866168357366572, 0.7881164487252263, 0.8285059714691135, 0.3408974641165834, 0.6151860325590366, 0.7819036016327547, 0.3780396288383874, 0.5707815255990233, 0.2237140727487692, 0.08174326235239371, 0.26672364298173634, 0.8907681278553053, 0.5644468332401974, 0.9250672021084733, 0.4577692590412453, 0.2771827661076983, 0.7870146635603288, 0.8277681566457297, 0.012381744486666624, 0.670411639023931, 0.09168312261651779, 0.1151024984279273, 0.8850600703796611, 0.04002353689016469, 0.2396333648675093, 0.9881584986060327, 0.4210135874302673, 0.1155581805922733, 0.16738343746133177, 0.24142028509784308, 0.7440064165370084, 0.1028341459863098, 0.9107644182793333, 0.3782772705442261, 0.9702640365282106, 0.9092227281507113, 0.29402358494854774, 0.2534101360411267, 0.47701009597226784, 0.10012914395045203, 0.6520501994894172, 0.039620213413704475, 0.010506151518672291, 0.9825836265504634, 0.2955498600489178, 0.5965706431884413, 0.44984453463009777, 0.31328086106892794, 0.06296479004764532, 0.9133920171659404, 0.9698132768381156, 0.9697965044964699, 0.1113623101268919, 0.21519327003609845, 0.6178068800115557, 0.979952885890077, 0.5429131974847156, 0.6881898080477126, 0.6618344288753493, 0.259085991853645, 0.5416022629129655, 0.3073211178125135, 0.24638119608509224, 0.08136876538378779, 0.2807867235646755, 0.9833767172194025, 0.4479022405332955, 0.6520105345126705, 0.6434660802698416, 0.940734522249, 0.39047855113892316, 0.3067842948515136, 0.3272414146871332, 0.3167351468856021, 0.847134765826215, 0.893500245521601, 0.3028093296725163, 0.33433340565076186, 0.5442254141821842, 0.5789854363170839, 0.5959625400010043, 0.2450980038952486, 0.020374028446252357, 0.24375929982791578, 0.07232753387141089, 0.551204754915506, 0.07091636753953445, 0.07512979225452299, 0.6353820935630572, 0.2908215504193956, 0.7921847578822924, 0.49326104275013793, 0.8626489777797094, 0.15417959616284405, 0.5014295859466933, 0.794983493746024, 0.0771069862639161, 0.9492279489729363, 0.1732421083716036, 0.7762089829859355, 0.9848958711440725, 0.8215501447435144, 0.3197840027930057, 0.1068777345815598, 0.5143582510552492, 0.919356939210688, 0.29348949437066774, 0.8937587976957898, 0.14168064702669492, 0.9104816743927341, 0.03175994589733666, 0.3160686777608829, 0.9030882837141124, 0.8038562809839719, 0.9071537669967973, 0.8407185222467378, 0.7461848854045222, 0.6895951793002646, 0.1781548656443236, 0.43263800097623695, 0.15789694375216057, 0.7148244519688113, 0.667778739685542, 0.2525864077938834, 0.0644141933476613, 0.9633858833215757, 0.8082526283723965, 0.5492699313925192, 0.5413776519849807, 0.8512926663313799, 0.45330967762221785, 0.39571044472076744, 0.33866914489505884, 0.2579690924717717, 0.024408502825104206, 0.6464388440000969, 0.4166838822984099, 0.5706036315777225, 0.062321630803521044, 0.3549434436862958, 0.13828411395509788, 0.12512901528549036, 0.259112968915828, 0.8289343809851581, 0.39779731306487276, 0.40108215192090135, 0.612444922992939, 0.23352965329584996, 0.007477173042134244, 0.5287017398867132, 0.5008996195572266, 0.6488395923408533, 0.4383169556417158, 0.6865131306582006, 0.7314219491610718, 0.23837467516202382, 0.4950722507160109, 0.47882688758179337, 0.225062085038767, 0.4122461329173408, 0.560407434487989, 0.9069395045058483, 0.9177065838382222, 0.27522536346579907, 0.6464151756425885, 0.0481973433614038, 0.07155138822789708, 0.5116917092002066, 0.877424078946487, 0.15946773075783105, 0.7660278587973122, 0.8830095693755464, 0.3118020318353023, 0.6925569646028146, 0.8489911224865752, 0.3716143307475649, 0.7012826629078087, 0.7364181165753182, 0.5945778048409015, 0.8562771389130047, 0.8966043711163488, 0.9600788169648591, 0.5712326942175455, 0.17627589520647535, 0.2505954088773793, 0.21761868850658306, 0.5695173495977943, 0.7577501146664367, 0.05213322114218644, 0.6816364556074682, 0.7171532633675107, 0.3479815079568077, 0.5150558042933419, 0.16479815203117487, 0.7298961504869986, 0.040708687336548866, 0.981221058148159, 0.8079437334476703, 0.6284485019821408, 0.2675262446471117, 0.9128628900924319, 0.9594388378770715, 0.13912615902147096, 0.7757572503157156, 0.8419308585435238, 0.6597173563139825, 0.7004077664167305, 0.44505873211451163, 0.9243078026249281, 0.9712075281962813, 0.3823533128201745, 0.8027115308003568, 0.4329215913805363, 0.16475421868327378, 0.32546727685726395, 0.1263300748348425, 0.9088847599027046, 0.9594240800441438, 0.11918673240587485, 0.6006790811870585, 0.40822409770858314, 0.11809003100178916, 0.295475514811817, 0.2482163710806481, 0.7495768111897567, 0.004008955954045934, 0.18983870393308366, 0.43877307011993694, 0.02103467308587126, 0.6275265885374804, 0.6056275385785042, 0.8353323508828638, 0.20660581568518466, 0.2847816135615888, 0.5423394307527486, 0.2732256972129319, 0.585738083402959, 0.25088222945000915, 0.6835271525859573, 0.7910907183680019, 0.8086546201638074, 0.9736161095498469, 0.5453770038258688, 0.49080927982901434, 0.8556976997986436, 0.7690673858593793, 0.5705446293870352, 0.3832563847662638, 0.2840474457335592, 0.10813920873416805, 0.8075490893732804, 0.11807153053066555, 0.7472652346880435, 0.545287089768146, 0.9649453287863279, 0.7610656598531885, 0.9735197845800538, 0.13659401293980755, 0.5003714738318865, 0.5725782871654547, 0.3112514573124735, 0.5030324882064976, 0.35681876360334597, 0.5283939713514435, 0.0008447179488895173, 0.4423143321124289, 0.4495521437392589, 0.3047991882212113, 0.3994027475965406, 0.7830873111719908, 0.6834128839628029, 0.4922991328917098, 0.6476682418421831, 0.377558211851013, 0.20391405043667976, 0.003875657877555727, 0.27762125160942186, 0.598164198713661, 0.8816629330706961, 0.8294212499885301, 0.5109602078711931, 0.987018145049427, 0.46158097386980335, 0.8345934861668383, 0.4089653412809712, 0.7446306177387316, 0.9875916912226816, 0.30533659236797617, 0.17031282521328428, 0.6200337087276608, 0.5309561803740346, 0.359422031985154, 0.003519242097051234, 0.3891626416098043, 0.4258694721036601, 0.405252071738319, 0.8612453089775505, 0.5844280270821319, 0.7338307924531678, 0.8979091716371104, 0.7487734635751375, 0.4927020519050469, 0.7457683402868462, 0.6403554004952637, 0.6487454346633404, 0.6296753586886549, 0.4069989749884928, 0.6292620312875881, 0.6337325109456275, 0.9371179595389777, 0.782473685370823, 0.8462680666010907, 0.7674997901425722, 0.8153258619910289, 0.6054623947302108, 0.3494500883866837, 0.26458325831813634, 0.7080200270648295, 0.8739420748131903, 0.5442467578028801, 0.1520699669575002, 0.8329752851974283, 0.48454307891146764, 0.4671026282781843, 0.04538805984571925, 0.5102809227900958, 0.7447476654547172, 0.4225978111457399, 0.3551773135885514, 0.6568435388988518, 0.01974138739808462, 0.5071635969746414, 0.9461270955326195, 0.6904475919384765, 0.40192372825721256, 0.6889082362934618, 0.6049939193159586, 0.2088893914825677, 0.2077083307298535, 0.8860252896990286, 0.2690692102056307, 0.07488477751012912, 0.8306775905962271, 0.5231977675764631, 0.3682081659729527, 0.5115189221326331, 0.7367256883512614, 0.16855360788759777, 0.6530669982365253, 0.713436998399841, 0.8150034439283779, 0.26976063367613834, 0.6096663306641944, 0.23211387837349717, 0.5610446736195358, 0.1723629719288945, 0.7897676248812812, 0.8667178646504996, 0.32964356032052855, 0.22231856181299336, 0.9637884170558321, 0.706690313251521, 0.8437926222446576, 0.030534474937409795, 0.8993933116527743, 0.6224520608976366, 0.3165291542410674, 0.43176562289240816, 0.761592993501026, 0.785411955930974, 0.18990086818143226, 0.6258865053379801, 0.16562952750215765, 0.9730498312350108, 0.44357655630583415, 0.913145005203284, 0.7282478447867935, 0.6062599043956083, 0.261984031344887, 0.5265923229048832, 0.13861974163698576, 0.13809799323879335, 0.7157497662356598, 0.36108976833344886, 0.7513763114866316, 0.2404936039137613, 0.7181581423147705, 0.7184769263967773, 0.3054958810525106, 0.10638543387964139, 0.3970078551871341, 0.49236150032733617, 0.09997421469778434, 0.18676126036778584, 0.055343052815480465, 0.5975135715550439, 0.8888761233719161, 0.2165577909596218, 0.03471343587681974, 0.7039235944191828, 0.8149105587896851, 0.9641215867338897, 0.6131789568237019, 0.34244316565189636, 0.8378686180306556, 0.11806710521312225, 0.6926369381896267, 0.0952308492516365, 0.3997057470173988, 0.49502288140217887, 0.377894273032341, 0.16859757880447968, 0.2317173126022275, 0.8201499974998944, 0.46257580479248983, 0.5799327447235099, 0.2119070176161595, 0.7149350587865332, 0.33011725914726364, 0.5936185874860408, 0.9094870627958156, 0.9943934088859884, 0.04621794831314552, 0.797442711928691, 0.8575878253608825, 0.3195744372072056, 0.3831476259821177, 0.5802537596763331, 0.9188402309707125, 0.39992859333804187, 0.8800301687734118, 0.7585605282041756, 0.1522730797062255, 0.9136799203638493, 0.015181052589951283, 0.1451782500468748, 0.6648112128866874, 0.05711968663889244, 0.3794898856741835, 0.12997885852693347, 0.4628892738532562, 0.8399803437546011, 0.9060843513491861, 0.03546964032188504, 0.060851756668864554, 0.8406240353653226, 0.0428147832556115, 0.273590265071345, 0.11743671769283648, 0.09103770695709379, 0.027622889724836064, 0.6375130126648525, 0.7446142679398566, 0.6867713765586763, 0.8456227719182262, 0.6630161884986934, 0.38970192767534384, 0.6310630237160113, 0.9695948083687032, 0.6416033330232526, 0.24309173409213014, 0.0601840957099572, 0.9351659997400953, 0.5904954982942084, 0.3496147426104088, 0.6053527496610309, 0.5602575960634735, 0.5221717727865457, 0.06080464202945668, 0.3532275523761348, 0.4126500229395509, 0.199368340608838, 0.880105231228507, 0.4241197773808294, 0.6623856654024448, 0.7135464494458958, 0.7432830602725053, 0.7211152909126985, 0.7522085016390995, 0.25158069415076423, 0.9764036766928967, 0.15100975378386006, 0.9186473950993009, 0.8545687752075629, 0.8521642911799676, 0.052811254837533905, 0.09121808344389948, 0.8130558022323219, 0.4691668264651879, 0.37025319113792565, 0.9846874722293574, 0.04011793528964003, 0.5314650538056048, 0.44334977615070714, 0.12820312302867765, 0.3951882627859874, 0.7076474048105019, 0.8823156092024081, 0.024619711463343408, 0.5245095586030891, 0.09037659503525841, 0.8003934571550348, 0.08578527943670455, 0.034193321017138345, 0.3842362020772886, 0.7326061745063001, 0.3132066930474475, 0.1300048996530475, 0.7945722220851718, 0.806919381895185, 0.8558597987725721, 0.30374447326405685, 0.42483036101897353, 0.24538999425425345, 0.5571774930165061, 0.33010716678974783, 0.3386633359590182, 0.7836214184097365, 0.9562961600402223, 0.5841403192367585, 0.10468793011995758, 0.6525749326846105, 0.4486117178480802, 0.988030557026313, 0.7193814951479868, 0.834786106507209, 0.701286260188212, 0.5356190057863918, 0.8968183918281254, 0.831617064708009, 0.291325887614329, 0.15703189522008743, 0.3703518687876949, 0.5210776725725857, 0.09738008983062874, 0.34537928645586036, 0.57490566421198, 0.043574618551851296, 0.8149486765188295, 0.651117045683278, 0.3136501715897636, 0.2983209812551685, 0.35261614078782044, 0.325288696205143, 0.7485137769587532, 0.5010568574712526], 'output': 1.0, 'delta': 0.0}, {'weights': [0.526128397299826, 0.14875649897091658, 0.9144180024177262, 0.32557292867233356, 0.32756445238821197, 0.06884613969783304, 0.9794115817517957, 0.4796978418092589, 0.9128847372842237, 0.9276172424974835, 0.9697521431783417, 0.8156292877315128, 0.9254432251913127, 0.9222893236500579, 0.8013676781661853, 0.13458121604268347, 0.5237117222858407, 0.5756040130041492, 0.9924975279861579, 0.7839485499662527, 0.7029162166549554, 0.7466490368444387, 0.36157776408347686, 0.9423135578402168, 0.6435008896152288, 0.4025746085300167, 0.46457157729760856, 0.9797549273107325, 0.5321283974315382, 0.1677975358744883, 0.14835499413404984, 0.6872421966577477, 0.5627755309150185, 0.9068062611875043, 0.18460034404937076, 0.41110881372687, 0.7279602186359784, 0.05010503390228793, 0.0992224065854852, 0.5457079014280206, 0.2657292165954248, 0.10693759623426746, 0.2616975684968622, 0.6321410877348209, 0.5263774368243828, 0.07849676054083088, 0.07281144555071173, 0.8506269918187016, 0.6432389604915947, 0.17336725824681098, 0.8618340673453347, 0.021849383341961626, 0.3681047923863917, 0.8476297370096515, 0.7102784127552225, 0.28375240579198935, 0.8912814945011249, 0.5980780012429903, 0.8654933191750928, 0.8927933740259835, 0.42544407734419154, 0.6756003377375025, 0.5444763147281303, 0.9447352378727902, 0.798160742835389, 0.725818500464358, 0.8140323746264132, 0.9981599522851606, 0.25656118547402607, 0.20136363065451268, 0.7467828134595477, 0.7703325106256943, 0.5142837977116697, 0.4870758136839637, 0.4037430704820498, 0.882696930394086, 0.796231877641984, 0.5845975982069754, 0.04011908435692091, 0.8511415942600505, 0.4584536776423547, 0.1897605282107142, 0.2993542752861643, 0.6913344758903868, 0.005507078325543091, 0.12004464732009834, 0.30265363687643365, 0.8871913551832168, 0.7468604394462109, 0.9707917256397661, 0.5430287394303667, 0.5719682275786375, 0.5513768068142746, 0.5256272138017167, 0.5420405711205759, 0.8185675511269973, 0.9533687347014597, 0.4083007693497043, 0.6299652426023742, 0.3077594075539877, 0.3019103864650702, 0.5063173505975552, 0.5862676586235078, 0.5499944669940258, 0.9765797032009982, 0.16297123769479815, 0.6366644129872755, 0.9945310087813287, 0.736135286551329, 0.5659085142333045, 0.36836315259984176, 0.40213888348307436, 0.9365230922325852, 0.8953304495737955, 0.6696762890989386, 0.8987478918617728, 0.9251636496933648, 0.8463435694934305, 0.3834161927467227, 0.4643646424409569, 0.7959075032289314, 0.37263302978751556, 0.7493638087232053, 0.4814203811335208, 0.33654130539639904, 0.456148287680224, 0.11650945606622187, 0.35449675578396944, 0.41519443056181304, 0.01816357668492674, 0.17207397382000555, 0.26023304736439834, 0.8578840280109546, 0.5895771368306654, 0.28714490644357715, 0.9977266968258558, 0.257920600019801, 0.5137883371656904, 0.7395197854992286, 0.6913205405598513, 0.4335026840560392, 0.7769976922420457, 0.48579410624104935, 0.7154650675477161, 0.49137654117752905, 0.9714946851276203, 0.7161799402916624, 0.09137723642916373, 0.12947012637659616, 0.9665147971332322, 0.22922837533180018, 0.026136048907525855, 0.25322374817515025, 0.47978705744969674, 0.9521685622554448, 0.3991299021963799, 0.7235055877822264, 0.8343625217899382, 0.08916201761131004, 0.6118919548006078, 0.9957843575691612, 0.5495959685614494, 0.5344861777266802, 0.3467025387811272, 0.9461053956418471, 0.9695992389771277, 0.10316984902710391, 0.5528338602115798, 0.41962922986529316, 0.6716461609387466, 0.11864663656894625, 0.26533429089601646, 0.2787533788311609, 0.47971293930054115, 0.7932828344714875, 0.8578475121235484, 0.7864236400590823, 0.6768068346699463, 0.08719275792238956, 0.38971707317075366, 0.6687016222424277, 0.2942477813509059, 0.5078183971193535, 0.905078361876925, 0.11615703706526204, 0.8538766539036542, 0.10582967213640748, 0.38636443476107696, 0.9053894035696106, 0.2012000617915647, 0.5207426269174332, 0.4166040326891619, 0.8879472832020315, 0.9920646960788638, 0.2885925611028892, 0.4924765425448292, 0.8950051502153464, 0.5447956764179482, 0.21462493977480868, 0.7596623124447222, 0.33708929912036656, 0.4859743721996205, 0.008561907394052604, 0.9889670441246217, 0.6572823624825372, 0.9258128470566863, 0.9686852820873311, 0.267533682707241, 0.5405359761823108, 0.4402512334277048, 0.7598552175178571, 0.8423856653329628, 0.22856016090522546, 0.2745646630997781, 0.7062615472551386, 0.4116430517162146, 0.13020153534647938, 0.19531058823852132, 0.56084931366165, 0.5984944470487219, 0.9600715716066204, 0.532779953140362, 0.6089807637733641, 0.1488547544618255, 0.4138019179564879, 0.2797912916552048, 0.6954228379264253, 0.2670572511205558, 0.2144003100600701, 0.3676843985841177, 0.4705490562443623, 0.3383949710142522, 0.6057321560302136, 0.18120366885667227, 0.8799102945666537, 0.694171364986253, 0.5347632180856879, 0.0581622757311645, 0.3260066399610153, 0.6901073689391266, 0.6450642776594175, 0.8119541778331114, 0.8915085435895681, 0.31536636965384823, 0.4937306827337068, 0.330041610586696, 0.12792226588170752, 0.14011709167323072, 0.2564694451170253, 0.08802876349734245, 0.538825533010267, 0.7029224414380715, 0.563072593302158, 0.6847667479227295, 0.22624800773903986, 0.19940434771043636, 0.5675748486809771, 0.8842855938364801, 0.4222645485970449, 0.004236644311168547, 0.0200516046712097, 0.30530459301328705, 0.6153742314894979, 0.08456543641575165, 0.2245103439701539, 0.680690553975496, 0.9849919442923573, 0.3410728072306086, 0.6011389845517673, 0.5184298334961871, 0.02312477768701582, 0.3298344116436186, 0.13944117809385492, 0.2508216790751341, 0.7699809830135035, 0.6812025798410788, 0.04102292915434891, 0.0773751220987744, 0.7249292209653437, 0.10320969894518073, 0.3170199859809295, 0.26933762825747554, 0.04976651342031979, 0.031169973897321013, 0.139034784777178, 0.3993272287551849, 0.9337057301405899, 0.6383781261094081, 0.24206099729136576, 0.6796441847743212, 0.27363318955870597, 0.515238016010762, 0.3218276870172574, 0.9486709096447534, 0.3523625204215367, 0.8035628034992964, 0.641192963154336, 0.8433255786143237, 0.6061603719535075, 0.8703849857380972, 0.4051629833211974, 0.679002691631347, 0.6206371614737384, 0.5277337094812512, 0.5644399778449616, 0.5357619817100272, 0.3937707193277419, 0.8983193875803986, 0.6327294059296804, 0.5491230721259409, 0.05393905639716545, 0.5085281141837572, 0.1751467230959347, 0.2150232188197363, 0.43461226876448, 0.5459568203984656, 0.25041213288033703, 0.27093438017989424, 0.5301463399957483, 0.4732340732669327, 0.403287483072823, 0.10375352013793404, 0.37347765318360016, 0.6544212622752213, 0.5441989404197219, 0.5447527137062477, 0.8438181117509362, 0.7231630497228716, 0.6845892413021832, 0.03041366203908369, 0.30812795792137815, 0.6824123198703762, 0.1557727759655333, 0.9134730441814907, 0.14192653951509626, 0.8791214438292662, 0.21626835677080958, 0.8415897548272803, 0.848229682202499, 0.3354647112272019, 0.8885923720325766, 0.15976779278818198, 0.8491095140212457, 0.38173454875596, 0.439717601281703, 0.11785978061485969, 0.6010052647079792, 0.26975582014987187, 0.6668793014210087, 0.7993879440342788, 0.6036840226733041, 0.008184809515470737, 0.9523352385289845, 0.9196811677159858, 0.6429353217227561, 0.37950634767851676, 0.5619137655369324, 0.8828120686199001, 0.4595288040516242, 0.7792182447906874, 0.5985589003506996, 0.42227922585653344, 0.9335265559713849, 0.40843090717594177, 0.6057791222780027, 0.05327428951253488, 0.47076386793806957, 0.03741423521997789, 0.7041328675848595, 0.0005902410461580132, 0.042065567014851646, 0.11112561514520136, 0.1395748967710433, 0.5080783647537448, 0.35628839992753547, 0.27090331005250146, 0.9836236057298181, 0.9089999196574396, 0.6548623394699247, 0.8020869677805449, 0.819708367418291, 0.24517343884360088, 0.8082860605552211, 0.23981162239268738, 0.5623565610644854, 0.35771700644490745, 0.15865919825735098, 0.7768544334216305, 0.916341667652535, 0.31369855569597016, 0.8797625357454809, 0.34625609407939617, 0.6575553612841176, 0.9957895941910351, 0.7720707350640242, 0.05566721124166507, 0.4348726676027125, 0.37630325823086663, 0.2939317953132611, 0.816135550597578, 0.44102019666278236, 0.6992402988527708, 0.634931136739595, 0.5189957852987459, 0.05603122220558798, 0.6730352499596889, 0.891383085497047, 0.17219943212743039, 0.6427444191716964, 0.4874393485070537, 0.3409845810940497, 0.7104267189017098, 0.9751989661364331, 0.021664682628631304, 0.897305758366094, 0.38323864066304636, 0.8338483568511473, 0.17471138683888743, 0.7165915908500881, 0.09969648903871986, 0.3356101660048567, 0.9699086794931329, 0.6566155049590191, 0.7845237603545003, 0.4613054301067149, 0.4711669710990031, 0.49262514450720307, 0.7731552918492094, 0.723249807482727, 0.19376810216897122, 0.44060439168556, 0.5420239204198886, 0.571428645314855, 0.9267709424082688, 0.8397471765072257, 0.14988123634414607, 0.3761207194225269, 0.10897250323749441, 0.02622382083834418, 0.0745859588783212, 0.18296553536353388, 0.7660771785454262, 0.6672214232537149, 0.7978709773342509, 0.2885034152713297, 0.15551101531303413, 0.9721002692327158, 0.8260249130855136, 0.9467820693149293, 0.01878707456910067, 0.3965474800085378, 0.6337982170632666, 0.7360745801995551, 0.9126506166783467, 0.5377317942344237, 0.39079239958264134, 0.005324017585244256, 0.8038632441272912, 0.9821579264325665, 0.9072464418329662, 0.6622685058344358, 0.3424754639148959, 0.23915025648517396, 0.7750196869400034, 0.9354293685991805, 0.9603260916542147, 0.1756073785996679, 0.5853527487931638, 0.5131182686750813, 0.4274251776610529, 0.7944006922875018, 0.9357823842440698, 0.7246248214709705, 0.7003058605196282, 0.690614518611634, 0.6535567045078392, 0.5367539828808665, 0.2479157030445568, 0.7794770186017971, 0.11909343724707233, 0.6438881683971543, 0.38698731429640454, 0.5599625415697017, 0.6414363444969299, 0.47892352972164387, 0.9780941122656858, 0.23919305039462202, 0.012168333089732086, 0.9552579884177682, 0.3120077212633888, 0.278072578630875, 0.41555904721243764, 0.5949667329579694, 0.9861145657425004, 0.7075246857607629, 0.31832021303921443, 0.5346882763244379, 0.44868549698652116, 0.501587113760744, 0.4176081981794526, 0.16761786266328338, 0.395484065253623, 0.3890890986351384, 0.2007194198324832, 0.8169186732056046, 0.3599909240617184, 0.1514863912720431, 0.5668743199071905, 0.8448434112605253, 0.780561072535501, 0.6220402649317941, 0.7310380068460375, 0.3361145774153067, 0.14271145506552207, 0.25500966051425156, 0.34935364413456904, 0.27913377110264137, 0.4677614049126817, 0.14903233165931407, 0.130261785975196, 0.2527238668942108, 0.19650369190022143, 0.8017006261598003, 0.537556824225385, 0.19841122286775725, 0.4292171054788668, 0.8719155657278634, 0.5776121477722593, 0.5539142523743498, 0.39131807320958134, 0.19583743872172898, 0.6254050875808675, 0.07714940721601782, 0.7861899485237686, 0.05752485268012175, 0.7463473111792467, 0.38262914432029493, 0.6824114332903526, 0.5910054042704707, 0.1291756754568837, 0.5385021012004435, 0.07416754906970224, 0.2412183124566043, 0.38166891142299064, 0.2856711685837189, 0.6617593520798355, 0.9868346854833971, 0.35686151496364316, 0.8385970978312445, 0.22509934230030493, 0.7093308876738105, 0.3477203659126339, 0.5353633261603788, 0.08858336146387946, 0.8273532189349466, 0.2088351376755534, 0.4634527491174777, 0.2902957931201211, 0.8102029533838505, 0.5925947286415035, 0.6151849357234862, 0.7547485637932494, 0.25489656342834177, 0.058248170108083475, 0.8285553737078101, 0.31560514986441923, 0.8122711266008682, 0.9566394159445416, 0.6291912482818915, 0.10329198921112503, 0.8539871307856776, 0.6334281234927437, 0.24589920598766768, 0.20787202942545968, 0.5077213153006307, 0.12156584793434377, 0.9060200824268411, 0.7078621924830589, 0.8192821811677478, 0.38382052377502096, 0.9231913053799073, 0.13395476947645024, 0.7162500513967016, 0.25460402462682086, 0.003631626946558053, 0.12089146531089001, 0.201544046298763, 0.7633452680909094, 0.37804995971211, 0.48203064162281584, 0.6135818304916332, 0.26766037224015604, 0.6384335843307868, 0.6715719302788205, 0.9213691544113192, 0.5028668212377829, 0.8552861244264475, 0.9677517210967089, 0.7688954149308205, 0.42119183688272654, 0.2719797975866193, 0.09773187837962227, 0.8310268136396308, 0.12960001965353074, 0.5595128984441713, 0.45393071885249103, 0.044846419158992346, 0.2143377691055881, 0.8228965828576935, 0.5386596159811745, 0.9243946249503633, 0.9079739842078218, 0.09402755705351773, 0.6781168114103044, 0.042658178854013684, 0.4226665707957995, 0.44177494338744194, 0.956872732737817, 0.5953175015896558, 0.19000060742607294, 0.5097473068893228, 0.5218288850825015, 0.19707458639680242, 0.35973135127600175, 0.8774946375642467, 0.9814709257866746, 0.7768663166801824, 0.06450150416074041, 0.9058766741439587, 0.45845943722277716, 0.8340560392335773, 0.17677987285910168, 0.14768464754370092, 0.9066622848699335, 0.28552344045904365, 0.043055426950175724, 0.501048200315799, 0.9905684580353415, 0.8354980615186305, 0.3962996385394406, 0.993073414265694, 0.7966701948025767, 0.8420658675763089, 0.6461069531835517, 0.3943813314133705, 0.9057097386732066, 0.4706292224006611, 0.9346421662649822, 0.5521910708222612, 0.9098574658614854, 0.47715640081037314, 0.42682078707669624, 0.5886823143731551, 0.3173104658366761, 0.14939761605954083, 0.5893324431460085, 0.8509629219538113, 0.27777624924381694, 0.8650214121278488, 0.7871289610182677, 0.7756758582665128, 0.41513018601399276, 0.9987565168726059, 0.790878236469853, 0.5756487964222792, 0.11350996819836934, 0.5738154912706415, 0.014381200827081053, 0.9022086883488681, 0.3366972575551538, 0.36834486387883225, 0.5508831816499049, 0.63746402688442, 0.5827270677250831, 0.4849252171533167, 0.6343552401942114, 0.8471422608166053, 0.4462093959337685, 0.5000793778829608, 0.8103469203716892, 0.003406069596069261, 0.1607104980189884, 0.32502993465104124, 0.21393738795923867, 0.8960099487021844, 0.14821622214901997, 0.10788676443678502, 0.31720096518691276, 0.5086407543782814, 0.8214808580281753, 0.9956510837481631, 0.8518696819228958, 0.6088375998175497, 0.03760190092730609, 0.06346449082754002, 0.6307360771793745, 0.8198823093654813, 0.26551240499762985, 0.9692190095562402, 0.5503873026658288, 0.573771199478443, 0.6186219162008204, 0.07491419992300219, 0.17038813907205697, 0.9361922960907023, 0.2672952146366093, 0.08329304401782134, 0.282428939274216, 0.7261461812340448, 0.26280857052543405, 0.2105816684575813, 0.27712940217334403, 0.48042161797818994, 0.7375490927111236, 0.301322965230045, 0.8735096217006009, 0.9758824199729277, 0.8220163698767596, 0.07512543760771095, 0.315458568481098, 0.9257857896092999, 0.8593843990279285, 0.13325329151192067, 0.4422243447164529, 0.3639424204756041, 0.7474696638978153, 0.028709642509242794, 0.3154769645444748, 0.7497795906946959, 0.8868701722994499, 0.04062634488104666, 0.5883534304951312, 0.6636085482644283, 0.8729168662865777, 0.4245794280199017, 0.9730496846376722, 0.19742578441529735, 0.11476261606396565, 0.13004550318256214, 0.58672377844334, 0.12244049997288653, 0.2665968124444289, 0.1963016523525778, 0.05529366702814731, 0.9623832662114691, 0.33492537533535827, 0.9640157641060344, 0.7232340153655419, 0.21976923887144084, 0.9325466799393285, 0.009351998671515926, 0.9816548190172133, 0.032264367970746055, 0.2533133469424036, 0.5519571977843915, 0.009177761004041995, 0.7647118187499721, 0.08465419355261006, 0.8170863477387695, 0.035104356302697926, 0.5281577125210466, 0.20943697126947192, 0.28876406880483374, 0.4904844128034528, 0.37137797716929866, 0.3919800590310131, 0.6534304773321632, 0.19524138863623164, 0.18150123348479186, 0.6843940066028997, 0.2969628569142482, 0.93295518741951, 0.42624009112762795, 0.47402111711708705, 0.023170243854374162, 0.02065552718963659, 0.10476783409087498, 0.6256280957010675, 0.6645434097928273, 0.952197558029905, 0.432469433749579, 0.7076705663069865, 0.3436021383024037, 0.07406193499957914, 0.42018544819424697], 'output': 1.0, 'delta': -0.0}]\n",
      "[{'weights': [-3.229258213193814, -3.1266587191827493, -2.9788989512060065], 'output': 0.009309120477086924, 'delta': -8.586172094881364e-05}, {'weights': [-2.0212176053658295, -2.2897761518755284, -2.303024556144417], 'output': 0.035331515167260685, 'delta': -0.0012058623542312093}, {'weights': [-2.1768073727830335, -2.2002960452716294, -1.9974110125402165], 'output': 0.03964808236534779, 'delta': -0.0015122235312262293}, {'weights': [-2.093614937965423, -1.8121594072713856, -2.2192470772718984], 'output': 0.04468041307044954, 'delta': -0.0019112270863401342}, {'weights': [-2.330781522929011, -1.9698771340327135, -2.126320167019121], 'output': 0.03866123609403936, 'delta': -0.001439244162025828}, {'weights': [-2.320452789919333, -2.281457282087299, -2.039199741924793], 'output': 0.034872727417529656, 'delta': -0.0011752678696598856}, {'weights': [-1.9609548871542941, -2.3091864880075272, -2.2580936788721946], 'output': 0.036822894257611226, 'delta': 0.03355393143660504}, {'weights': [-1.9973039470971006, -2.556060819925843, -2.144332551990752], 'output': 0.033932886519014396, 'delta': -0.0011137809982797657}, {'weights': [-1.6396560299161766, -2.2994190892905007, -2.087406845265904], 'output': 0.04683126174048252, 'delta': -0.0020953508439555055}, {'weights': [-2.3015820441445447, -2.0755828537872834, -2.3048134092850763], 'output': 0.03419147839078414, 'delta': -0.001130539521102068}, {'weights': [-2.0742405058482176, -2.316175917558548, -2.262711600296971], 'output': 0.0346710455634095, 'delta': -0.0011619388036645498}, {'weights': [-2.1316229146722114, -2.417249961195392, -1.9886985626500153], 'output': 0.036657685144536954, 'delta': -0.0012964303354105458}, {'weights': [-2.610946063376203, -1.867719063696256, -1.8770899233123362], 'output': 0.04000677066705912, 'delta': -0.0015391790851860979}, {'weights': [-1.945753624953363, -2.2372328999881828, -1.9140349094489066], 'output': 0.04528181363743118, 'delta': -0.0019618951032773437}, {'weights': [-2.3672474747602705, -2.1694862226396094, -2.0860670768043437], 'output': 0.03518215299497815, 'delta': -0.0011958603860016106}, {'weights': [-2.0935307554364844, -2.2203570818957274, -2.1822077970051286], 'output': 0.03739709967826879, 'delta': -0.001348299111183672}, {'weights': [-1.9042288561632366, -2.2478404747023712, -2.764912312524102], 'output': 0.03051665126438574, 'delta': -0.0009037817065802012}, {'weights': [-2.2079669842382295, -1.939254388273474, -2.2912951788080873], 'output': 0.03844739706766419, 'delta': -0.001423659308810858}, {'weights': [-2.564613330515009, -1.8347958982243824, -2.096632046683164], 'output': 0.03739807811180654, 'delta': -0.0013483683962239347}, {'weights': [-2.5882148299245338, -2.0049811530645227, -2.2670244429563344], 'output': 0.03136758340194944, 'delta': -0.0009541023238385226}, {'weights': [-1.808881280503369, -2.017137035944643, -2.089172742915483], 'output': 0.04937875088466327, 'delta': -0.002323855071563213}, {'weights': [-2.0271254003476464, -1.9211597484338785, -1.9861189627811908], 'output': 0.0489297629228825, 'delta': -0.002282764510717463}, {'weights': [-1.7015203392871787, -2.493580445666318, -2.3687966729337684], 'output': 0.036195672423429505, 'delta': -0.001264519077042994}, {'weights': [-1.904354457645716, -2.2340016126727082, -2.2243298933113502], 'output': 0.03987388750678762, 'delta': -0.0015291661787948108}, {'weights': [-2.1508440460080975, -2.537098852164585, -2.3859147651696877], 'output': 0.028279560990464282, 'delta': -0.0007778123323819934}, {'weights': [-2.365916841418276, -2.424082018389438, -1.5865529881024214], 'output': 0.03960931208571287, 'delta': -0.0015093236409055735}, {'weights': [-1.9987853702158196, -1.8634622283927296, -2.7411362465927187], 'output': 0.03551319196614446, 'delta': -0.0012180824600012824}]\n",
      "Expected=23, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=21, Got=20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected=20, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=15, Got=20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected=15, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=25, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=23, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=4, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=7, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=8, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=22, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=16, Got=20\n",
      "Expected=10, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=13, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=5, Got=20\n",
      "Expected=20, Got=20\n",
      "Expected=15, Got=20\n",
      "Expected=24, Got=20\n",
      "Expected=14, Got=20\n",
      "Expected=9, Got=20\n",
      "Expected=17, Got=20\n",
      "Expected=21, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=26, Got=20\n",
      "Expected=12, Got=20\n",
      "Expected=6, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=18, Got=20\n",
      "Expected=11, Got=20\n",
      "Expected=2, Got=20\n",
      "Expected=19, Got=20\n",
      "Expected=3, Got=20\n",
      "Expected=1, Got=20\n",
      "Expected=6, Got=20\n"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import randrange\n",
    "from random import random\n",
    "from math import exp\n",
    "import random\n",
    "import numpy as np\n",
    "from emnist import extract_training_samples\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "T = (-1)\n",
    "alpha = 0.5\n",
    "\n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    stats = [[min(column), max(column)] for column in zip(*dataset)]\n",
    "    return stats\n",
    "\n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += (weights[i] * inputs[i])\n",
    "    return activation\n",
    "\n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-alpha * activation))\n",
    "\n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    "\n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)\n",
    "\n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "\n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']\n",
    "\n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "\n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random.random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random.random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))\n",
    "\n",
    "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
    "    n_inputs = len(train[0]) - 1\n",
    "    n_outputs = len(set([row[-1] for row in train]))\n",
    "    network = initialize_network(n_inputs, n_hidden, n_outputs+1)\n",
    "    train_network(network, train, l_rate, n_epoch, n_outputs+1)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        prediction = predict(network, row)\n",
    "        predictions.append(prediction)\n",
    "    return(predictions)\n",
    "\n",
    "# Test Backprop on Seeds dataset\n",
    "seed(1)\n",
    "\n",
    "images, labels = extract_training_samples('letters')\n",
    "data = images[:1000].tolist()\n",
    "\n",
    "dataset = list()\n",
    "for i in range(len(data)):\n",
    "    value = np.concatenate(data[i]).tolist()\n",
    "    for j in range(len(value)):\n",
    "        if(value[j] != 0):\n",
    "            value[j] = 1\n",
    "            \n",
    "    value.append(labels[i])\n",
    "    dataset.append(value)\n",
    "\n",
    "first_image = np.array(images[3], dtype='float')\n",
    "print(labels[3])\n",
    "pixels = first_image.reshape((28, 28))\n",
    "plt.imshow(pixels, cmap='gray')\n",
    "plt.show()\n",
    "    \n",
    "#print(images)\n",
    "#print(labels)\n",
    "\n",
    "#print(dataset)\n",
    "n_inputs = len(dataset[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "\n",
    "#network = initialize_network(n_inputs, 2, n_outputs+1)\n",
    "#train_network(network, dataset, 0.4, 10, n_outputs+1)\n",
    "for layer in network:\n",
    "    print(layer)\n",
    "\n",
    "for row in dataset:\n",
    "    prediction = predict(network, row)\n",
    "    print('Expected=%d, Got=%d' % (row[-1], prediction))\n",
    "    \n",
    "#scores = evaluate_algorithm(dataset, back_propagation, n_inputs, 0.4, 10, n_outputs+1)\n",
    "#print('Scores: %s' % scores)\n",
    "#print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12e4ece9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf802a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "356eaec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from mnist import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23d3ca6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4807700fdd1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0memnist_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gzip\\\\'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'numpy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0memnist_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_emnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'letters'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx_orig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_orig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memnist_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_orig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "emnist_data = MNIST(path='gzip\\\\', return_type='numpy')\n",
    "emnist_data.select_emnist('letters')\n",
    "x_orig, y_orig = emnist_data.load_training()\n",
    "\n",
    "train_x = x_orig[0:3000, :]\n",
    "Y = y_orig.reshape(y_orig.shape[0], 1)\n",
    "Y = Y[0:3000, :]\n",
    "Y = Y[:, 0]\n",
    "train_y = (np.arange(np.max(Y) + 1) == Y[:, None]).astype(int)\n",
    "\n",
    "X_test = x_orig[3000:3500, :]\n",
    "Y_test = y_orig.reshape(y_orig.shape[0], 1)\n",
    "Y_test = Y_test[3000:3500, :]\n",
    "Y_test = Y_test[:, 0]\n",
    "\n",
    "letter_count = {0: 'CHECK', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j',\n",
    "                11: 'k',\n",
    "                12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v',\n",
    "                23: 'w',\n",
    "                24: 'x', 25: 'y', 26: 'z', 27: 'CHECK'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c1fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z), axis=1))\n",
    "    return sm\n",
    "\n",
    "\n",
    "def initialize(dim1, dim2):\n",
    "    w = np.zeros(shape=(dim1, dim2))\n",
    "    b = np.zeros(shape=(27, 1))\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    m = X.shape[1]  # getting no of rows\n",
    "\n",
    "    # Forward Prop\n",
    "    A = softmax((np.dot(w.T, X) + b).T)\n",
    "    cost = (-1 / m) * np.sum(Y * np.log(A))\n",
    "\n",
    "    # backwar prop\n",
    "    dw = (1 / m) * np.dot(X, (A - Y).T)\n",
    "    db = (1 / m) * np.sum(A - Y)\n",
    "\n",
    "    cost = np.squeeze(cost)\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    return grads, cost\n",
    "\n",
    "\n",
    "def optimize(w, b, X, Y, num_iters, alpha, print_cost=False):\n",
    "\n",
    "    costs = []\n",
    "    for i in range(num_iters):\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        w = w - alpha * dw\n",
    "        b = b - alpha * db\n",
    "\n",
    "        # Record the costs\n",
    "        if i % 50 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 50 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "        if i>500:\n",
    "            alpha=alpha*0.9\n",
    "\n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "\n",
    "    return params, grads, costs\n",
    "\n",
    "\n",
    "def predict(w, b, X):\n",
    "    # m = X.shape[1]\n",
    "    # y_pred = np.zeros(shape=(1, m))\n",
    "    # w = w.reshape(X.shape[0], 1)\n",
    "\n",
    "    y_pred = np.argmax(softmax((np.dot(w.T, X) + b).T), axis=0)\n",
    "    return y_pred\n",
    "\n",
    "def predict_for_cv(w, b, X):\n",
    "    # m = X.shape[1]\n",
    "    # y_pred = np.zeros(shape=(1, m))\n",
    "    # w = w.reshape(X.shape[0], 1)\n",
    "\n",
    "    y_pred = np.argmax(softmax((np.dot(w.T, X) + b).T), axis=0)\n",
    "    return int(y_pred[0])\n",
    "\n",
    "def model(X_train, Y_train, Y,X_test,Y_test, num_iters, alpha, print_cost):\n",
    "    w, b = initialize(X_train.shape[0], Y_train.shape[0])\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iters, alpha, print_cost)\n",
    "\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "\n",
    "\n",
    "    y_prediction_train = predict(w, b, X_train)\n",
    "    y_prediction_test = predict(w, b, X_test)\n",
    "    print(\"Train accuracy: {} %\", sum(y_prediction_train == Y) / (float(len(Y))) * 100)\n",
    "    print(\"Test accuracy: {} %\", sum(y_prediction_test == Y_test) / (float(len(Y_test))) * 100)\n",
    "\n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": y_prediction_test,\n",
    "         \"Y_prediction_train\": y_prediction_train,\n",
    "         \"w\": w,\n",
    "         \"b\": b,\n",
    "         \"learning_rate\": alpha,\n",
    "         \"num_iterations\": num_iters}\n",
    "\n",
    "    # Plot learning curve (with costs)\n",
    "    costs = np.squeeze(d['costs'])\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "    plt.plot()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    #pri(X_test.T, y_prediction_test)\n",
    "    return d\n",
    "\n",
    "\n",
    "def pri(X_test, y_prediction_test):\n",
    "    example = X_test[2, :]\n",
    "    print(\"Prediction for the example is \", y_prediction_test[2])\n",
    "    plt.imshow(np.reshape(example, [28, 28]))\n",
    "    plt.plot()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fdd7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = model(train_x.T, train_y.T, Y, X_test.T, Y_test, num_iters=800, alpha=0.00009,\n",
    "                                      print_cost=True)\n",
    "w_LR = d1[\"w\"]\n",
    "b_LR = d1[\"b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df5ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b9c62e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-d6cc71cb43cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[0mn_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m \u001b[0mn_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitialize_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_outputs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-d6cc71cb43cd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[0mn_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m \u001b[0mn_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitialize_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_outputs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import randrange\n",
    "from random import random\n",
    "from math import exp\n",
    "import random\n",
    "import numpy as np\n",
    "from emnist import extract_training_samples\n",
    "\n",
    "T = (-1)\n",
    "alpha = 0.5\n",
    "\n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    stats = [[min(column), max(column)] for column in zip(*dataset)]\n",
    "    return stats\n",
    "\n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += (weights[i] * inputs[i])\n",
    "    return activation\n",
    "\n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-alpha * activation))\n",
    "\n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    "\n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)\n",
    "\n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "\n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']\n",
    "\n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "\n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random.random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random.random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))\n",
    "\n",
    "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
    "    n_inputs = len(train[0]) - 1\n",
    "    n_outputs = len(set([row[-1] for row in train]))\n",
    "    network = initialize_network(n_inputs, n_hidden, n_outputs+1)\n",
    "    train_network(network, train, l_rate, n_epoch, n_outputs+1)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        prediction = predict(network, row)\n",
    "        predictions.append(prediction)\n",
    "    return(predictions)\n",
    "\n",
    "# Test Backprop on Seeds dataset\n",
    "seed(1)\n",
    "\n",
    "images, labels = extract_training_samples('letters')\n",
    "\n",
    "dataset = images.tolist()\n",
    "for i in range(len(dataset)):\n",
    "    dataset.append(labels[i])\n",
    "    \n",
    "print(dataset)\n",
    "\n",
    "n_inputs = len(dataset[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "\n",
    "network = initialize_network(n_inputs, 2, n_outputs+1)\n",
    "train_network(network, dataset, 0.4, 10, n_outputs+1)\n",
    "for layer in network:\n",
    "    print(layer)\n",
    "\n",
    "for row in dataset:\n",
    "    prediction = predict(network, row)\n",
    "    print('Expected=%d, Got=%d' % (row[-1], prediction))\n",
    "    \n",
    "scores = evaluate_algorithm(dataset, back_propagation, n_inputs, 0.4, 10, n_outputs+1)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf7fbb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
